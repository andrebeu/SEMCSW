{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nick comment on run model\n",
    "The main script we use to run the experiment is  `schema_prediction_task_9_8_20.batch_exp`.  It's a wrapper function for a lot of code,\n",
    "but it will generate a set of randomly sampled tasks of the specified conditions (default = Blocked, Interleaved), run the model (either SEM or a \"no_split\" variant of SEM that collapses to the NN specified) and calculate the meaningful metrics of the task.\n",
    "\n",
    "\n",
    "It's desined to be a parallelizable job on a cluster -- you can pass a set of parameters to the function and get a pair of files as an output that represents a random sample of behavior for those parameters on the specified tasks.  I (NTF) would spawn thousands of instances of this function in seperate SLURM jobs, each with different parameters and write the results to file for grid-searches.  It is possible to get fancier with the paralization by writting a custom wrapper for some of the SEM module (look at the function `no_split_sem.no_split_sem_run_with_boundaries` for ideas) but I chose not to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQ9KSajiwUpk"
   },
   "source": [
    "This script runs one param_set\n",
    "outputs results and trialxtrial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCrz6mCuD6i8"
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.7.9 (default, Aug 31 2020, 07:22:35) \n",
      "[Clang 10.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import norm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "from schema_prediction_task_9_8_20 import generate_exp, batch_exp\n",
    "from vanilla_lstm import VanillaLSTM\n",
    "from sem.event_models import NonLinearEvent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gridsearch params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'LSTM'\n",
    "lr                = 0.05\n",
    "n_epochs          = 2    # what does this control?\n",
    "log_alpha         = 0.0  # sCRP alpha is set in log scale\n",
    "log_lambda        = 0.0  # sCRP lambda is set in log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle between SEM (False) and LSTM (True)\n",
    "if model_type == 'SEM-LSTM':\n",
    "  no_split=False\n",
    "elif model_type == 'LSTM':\n",
    "  no_split=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_weight & instructions_weight control instruction condition\n",
    "story_kwargs = dict(seed=None, err=0.2, actor_weight=1.0, instructions_weight=1.0)\n",
    "\n",
    "## to inspect embeddings, use:\n",
    "# x, y, e, embedding_library = generate_exp('blocked', **story_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sem parameters\n",
    "n_hidden          = None # don't change -- set the number of hidden units equal to the dimensionality\n",
    "batch_update      = False # batch update (True) or local gradient updating (False)\n",
    "\n",
    "dropout           = 0.0  # don't change\n",
    "l2_regularization = 0.0  # don't change\n",
    "batch_size        = 25   # don't change\n",
    "epsilon           = 1e-5 # don't change\n",
    "\n",
    "\n",
    "optimizer_kwargs = dict(\n",
    "    lr=lr, beta_1=0.9, beta_2=0.999, epsilon=epsilon, amsgrad=False\n",
    ") \n",
    "\n",
    "f_opts=dict(\n",
    "  batch_size=batch_size, \n",
    "  batch_update=batch_update, \n",
    "  dropout=dropout,\n",
    "  l2_regularization=l2_regularization, \n",
    "  n_epochs=n_epochs,\n",
    "    optimizer_kwargs=optimizer_kwargs\n",
    ")\n",
    "\n",
    "f_class = VanillaLSTM # event model class\n",
    "\n",
    "# final param dict\n",
    "sem_kwargs = dict(\n",
    "  lmda=np.exp(log_lambda), \n",
    "  alfa=np.exp(log_alpha), \n",
    "  f_opts=f_opts, \n",
    "  f_class=f_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model\n",
    "\n",
    "main fun call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"condition params\"\"\"\n",
    "\n",
    "block_only=True \n",
    "run_instructed=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run SEM: 100%|██████████| 200/200 [01:28<00:00,  2.26it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "batch_exp main fun call\n",
    "\"\"\"\n",
    "\n",
    "n_batch = 1 # remove or debug param?\n",
    "print('n_batch',n_batch)\n",
    "results, trialXtrial, _ = batch_exp(\n",
    "              sem_kwargs, story_kwargs, \n",
    "              no_split=no_split, block_only=block_only, run_instructed=run_instructed, \n",
    "              sem_progress_bar=True, progress_bar=False,\n",
    "              n_batch=n_batch, \n",
    ")\n",
    "\n",
    "\n",
    "# convert from JSON file format (dict) to pandas df\n",
    "results = pd.DataFrame(results)\n",
    "trialXtrial = pd.DataFrame(trialXtrial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAOkKQ5O8j7f"
   },
   "source": [
    "# save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     Trials  adjRand  nClusters        pe  pe (probes)  verb decoder Accuracy  \\\n",
       " 0       All      0.0          1  0.566003     0.581288               0.107500   \n",
       " 1  Training      0.0          1  0.564398     0.580805               0.115625   \n",
       " 2      Test      0.0          1  0.572425     0.583218               0.075000   \n",
       " \n",
       "    verb decoder Accuracy Prob  verb 2 AFC decoder Prob  batch Condition  \\\n",
       " 0                    0.117275                 0.496754      0   Blocked   \n",
       " 1                    0.124738                 0.500790      0   Blocked   \n",
       " 2                    0.087423                 0.480609      0   Blocked   \n",
       " \n",
       "    cluster re-use  \n",
       " 0             NaN  \n",
       " 1             NaN  \n",
       " 2             1.0  ,\n",
       "        t  e_hat  Accuracy        pe  batch Condition\n",
       " 0      0      0  0.491652  0.619152      0   Blocked\n",
       " 1      1      0  0.163109  0.532164      0   Blocked\n",
       " 2      2      0  0.294964  0.717002      0   Blocked\n",
       " 3      3      0  0.454531  0.613671      0   Blocked\n",
       " 4      4      0  0.428602  0.643967      0   Blocked\n",
       " ..   ...    ...       ...       ...    ...       ...\n",
       " 195  195      0  0.291911  0.578051      0   Blocked\n",
       " 196  196      0  0.563588  0.667875      0   Blocked\n",
       " 197  197      0  0.111822  0.822531      0   Blocked\n",
       " 198  198      0  0.522030  0.444441      0   Blocked\n",
       " 199  199      0  0.586195  0.755779      0   Blocked\n",
       " \n",
       " [200 rows x 6 columns])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_fpath = None\n",
    "results,trialXtrial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params needed\n",
    "model_name\n",
    "nhidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#online version or batch update?\\n\\n    # run both the mixed or just blocked/interleaved?\\n    mixed = False\\n\\n    # run the instructed case?\\n    instructed = True\\n    \\n    # run the LSTM version or the MLP?\\n    LSTM = True\\n\\n    # Save the Prediction error and boundary info? (storage intensive)\\n    condensed_output = True\\n\\n    # dont' change these.\\n    #   Extensive testing says these values are fine and relatively unimportant!\\n    n_hidden = None\\n    epsilons = [1e-5]  \\n\\n    # parameter search over lr, n_epochs, alpha, lambda\\n    lrs = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\\n    n_epochs_ = [1, 2, 4, 8, 16, 32, 64]\\n    log_alphas = [-32, -16, -8, -4, -2, 0, 2, 4, 8, 16, 32]\\n    log_lambdas = [-32, -16, -8, -4, -2, 0, 2, 4, 8, 16, 32]\\n    \\n    # How many batches per simulation? Should be kept low for parameter searches\\n    n_batches = 50\\n    \\n    \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#online version or batch update?\n",
    "\n",
    "    # run both the mixed or just blocked/interleaved?\n",
    "    mixed = False\n",
    "\n",
    "    # run the instructed case?\n",
    "    instructed = True\n",
    "    \n",
    "    # run the LSTM version or the MLP?\n",
    "    LSTM = True\n",
    "\n",
    "    # Save the Prediction error and boundary info? (storage intensive)\n",
    "    condensed_output = True\n",
    "\n",
    "    # dont' change these.\n",
    "    #   Extensive testing says these values are fine and relatively unimportant!\n",
    "    n_hidden = None\n",
    "    epsilons = [1e-5]  \n",
    "\n",
    "    # parameter search over lr, n_epochs, alpha, lambda\n",
    "    lrs = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    n_epochs_ = [1, 2, 4, 8, 16, 32, 64]\n",
    "    log_alphas = [-32, -16, -8, -4, -2, 0, 2, 4, 8, 16, 32]\n",
    "    log_lambdas = [-32, -16, -8, -4, -2, 0, 2, 4, 8, 16, 32]\n",
    "    \n",
    "    # How many batches per simulation? Should be kept low for parameter searches\n",
    "    n_batches = 50\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter search over lr, n_epochs, alpha, lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEM-lr-0.050-nepchs-2-alpha-0.000-lambda-0.000\n"
     ]
    }
   ],
   "source": [
    "model_name = \"SEM\"\n",
    "model_tag = \"%s-lr-%.3f-nepchs-%i-alpha-%.3f-lambda-%.3f\"%(\n",
    "  model_name,lr,n_epochs,log_alpha,log_lambda\n",
    ")\n",
    "print(model_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fpath = \"results_\" + model_tag + '.csv'\n",
    "trial_fpath = \"trial_X_trial_\" + model_tag + '.csv'\n",
    "\n",
    "save_dir = ''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Fg0K8FUow9Dy",
    "J0shpNw-wRT2"
   ],
   "name": "AndreTask; 2/7/20.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
